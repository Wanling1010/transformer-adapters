# Transformer-Adapters
Studies show that domain adaptive pre-training (DAPT) and task adaptive pre-training (TAPT) of RoBERTa model
provides better results than task fine-tuning alone. Following the Gururanguan et.al paper, fine-tuned the DAPT cs_roberta_base model on
different tasks/datasets such as the ACL-ARC and SCIERC data, by employing different adapter structures from AdapterHub.
